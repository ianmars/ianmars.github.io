<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>  
    div.padded {  
      padding-top: 0px;  
      padding-right: 100px;  
      padding-bottom: 0.25in;  
      padding-left: 100px;  
    }  
  </style> 
<title>Ian Marshall  |  CS 184</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="style.css" media="screen" />
</head>
<body>
<br />
<h1 align="middle">Assignment 3: PathTracer</h1>
    <h2 align="middle">Ian Marshall</h2>


    <h2 align="middle">Part 1: Ray Generation and Intersection</h2>
        <p>The first step in tracing a ray is generating a ray to trace. Each pixel has some number of rays going through it out into the world from the camera. If only one ray passes throught the pixel we will have it pass through the center (i.e. if the pixel is [x, y] then the ray will pass through [x+0.5, y+0.5].) If we are having multiple sample rays per pixel then we will sample uniformly in [0, 1]^2 and add this to [x. y] to get the point the ray passes through. Our camera wants these values to be in the range [0, 1]^2 so in order to convert the pixel values into this range we then divide by the sample Buffer's width and height in pixels. Now the camera can use the value we've generated to make a ray. The camera is situated at the origin looking down the negative z-axis towards a viewing window that is given by corners</p>
        <pre><code>Vector3D(-tan(radians(hFov)*.5), -tan(radians(vFov)*.5),-1)
Vector3D( tan(radians(hFov)*.5),  tan(radians(vFov)*.5),-1)
</code></pre>
<p> so we should scale our x and y values into this range. In order to do this I determined the width and height of this viewing window. Then the coordinates of the pixel in the viewing window are the bottom left corner plus x times the width and y times the height. We can apply the given transform <code>o2w</code> to our point to get the direction vector of our ray which we then normalize. The origin is clearly the camera's <code>pos</code> and then the min and max t values should be set to the camera's <code>nclip</code> and <code>fclip</code>. After generating a ray, we need to be able to detect when it intersects with something in the environment. The two primitives we implemented were triangles and spheres. To detect triangle intersections I implemented the Moller Trumbore algorithm which takes advantage of barycentric coordinates to put the intersection point in terms of edges of the triangle and the ray's origin and direction and then solves this efficiently using rules about determinants. One intersect method only cared about whether an intersection occured and updating the ray's <code>max_t</code> accordingly. The other also updated an <code>isect</code> object that stores information about the t-value of the intersection as well as the surface normal at the intersection and the primitive that was intersected with and the bsdf of the intersected surface. For spheres we implemented two similar methods. Using the quadratic formuala I solved the equation</p>
<p align="middle">
    <pre align="middle">at^2 + bt + c = 0</pre>
<p>where</p> 
<p align="middle">
<pre align="middle">
    <code>a = dot(d, d)
b = dot(2(o - c), d)
c = dot((o - C), (o - C)) - R^2</code>
</pre>
</p>
<p> and C and R are the center and radius of the sphere. The ray intersected the sphere as long as the quadratic equation had a real solution. Then the intersection was the closer of the two points on the sphere that the ray hit.</p>
        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/CBspheres_screenshot_3-7_0-13-52.png" width="480px" />
                    <figcaption align="middle">Just a coupla fun spheres.</figcaption>
                </tr>
                <tr>
                    <td align="middle">
                        <img src="images/CBgems_screenshot_3-7_0-17-45.png" width="480px">
                        <figcaption align="middle">Some spooky gems.</figcaption>
                    </td>
                </tr>
            </table>
        </div>
<br>
<h2 align="middle">Part 2: Bounding Volume Hierarchy</h2>
<p>The first step in creating my BVH is to create a bbox that contains all the primitives in the image as well as a bbox that contains their centroids. Next if the number of prims is less than or equal to the maximum leaf size we make this a leaf node with a list of the prims. If not we divide the prims into left and right nodes based on their centroid position, we split along the longest axis of the centroid bounding box and then primitives whose centroid position is greater than the centroid bounding box's centroid go in the right node and the rest in the left node. This allows us to efficiently render larger images with many primitives when we use the right intersect algorithm. The intersect algorithm goes like this, first we check whether the ray intersects with the nodes bounding box, if it doesn't we're finished if it does then we check whether this is a leaf node or not. If it is a leaf node we check all the primitives for intersection otherwise we check whther the left and right child nodes have intersections. To check a bounding box for intersection we can take advantage of the fact that we have constructed them such that they are axis aligned. We can calculate two intersects for each axis corresponding to the planes defining the box in those directions or possibly one if the ray originates from inside the box. Then the maximum of the minimum values for each direction and the minimum of the maximum values for each direction define the interval during which the ray is within the box if that <code>tmin <= tmax</code>. As long as this interval intersects the ray's <code>min_t</code> to <code>max_t</code> interval then we can safely say that there was an intersection and update the ray and intersection data.</p>
<br>
<div align="center">
    <table style="width=100%">
        <tr>
            <td align="middle">
                <img src="images/bvh.png" align="middle" width="480px">
                <figcaption align="middle">Boxes......</figcaption>
            </td>
        </tr>
        <br>
        <tr>
            <td align="middle">
                <img src="images/CBlucy_screenshot_3-7_0-40-57.png" align="middle" width="480px">
                <figcaption align="middle">Lucy had way too many triangles to render.</figcaption>
            </td>
        </tr>
        <br>
        <tr>
            <td>
                <img src="images/maxplanck_screenshot_3-7_11-3-51.png" align="middle" width="480px">
                <figcaption align="middle">Hi Max.</figcaption>
            </td>
        </tr>
    </table>
</div>
<br>
<h2 align="middle">Part 3: Direct Illumination</h2>
<p>The next step of this assignment was to calculate the direct lighting at a point in the scene. We do this on a per light basis by iterating over the lights in the scene and taking their contributions to the light at a point. We are given many helpful utilities for converting vectors from object to world space and back to make this whole thing easier. If the light is a delta light (i.e. a point light source) then we only ask for one smaple since all samples would be to the sample point otherwise we take <code>ns_area_light</code> samples and average the contributions of each. When we figure out a lights contribution there are a few things we have to do. First we figure out its radiance at the point by using <code>sample_L</code> which gives us the lights spectrum as well as a distance to the light and the direction from the point to the sampled point on the light and the pdf of the light evaluated at that point we convert this radiance to irradiance later using a cosine term. Next we want to make sure that the <code>cos(wi)</code> is nonegative. If that's the case then we want to make a shadow ray to make sure that there are no objects between the light source and the surface that would block that light. If there is no shadow we can ask about the bsdf at the surface which governs how light coming in at some angle and going out at another behaves using the <code>wi</code> returned by <code>sample_L</code> converted to object coordinates with the transform <code>w2o</code>. For our simple diffuse bsdf lights has equal probability of going out in any direction so that's pretty easy its just the total reflected spectrum divided by the integral over the hemisphere of the probability. Finally we put all this information together by multiplying the incoming light by the bsdf and the cosine of the angle at which it hits the surface and then for an area light divide it by the pdf of that light sample and the number of samples.</p>

<div align="center">
    <table style="width=100%">
        <tr>
            <td>
                <img src="images/CBspheres_lambertian_screenshot_3-7_12-34-13.png" align="middle" width="480px">
                <figcaption align="middle">Spheres rendered with 8 light samples and 8 samples per pixel.</figcaption>
            </td>
        </tr>
        <br>
        <tr>
            <td>
                <img src="images/CBbunny_screenshot_3-7_12-39-7.png" align="middle" width="480px">
                <figcaption align="middle">A nice bun with 8 light samples and 8 samples perp pixel.</figcaption>
            </td>
        </tr>
        <br>
        <tr>
            <td>
                <img src="images/dragon_screenshot_3-7_13-0-18.png" align="middle" width="480px">
                <figcaption align="middle">A dragon with some sparkles on it : (</figcaption>
            </td>
        </tr>
    </table>
</div>
<br>
<p>One glaring flaw in all of this si that these objects are not casting shadows on the ground. What's up with that? I tried very hard to find this bug for almost a week checking code in many places particularly in my intersection method for bboxes but I was unsuccessful in fixing it. I am fairly certain that my logic for intersections with rays that originate within a bbox is correct but I've heard that that may be a problem. Also stemming from the shadow ray problem probably sre the sparkles all over my dragon which otherwise is shaded pretty nicely.</p>
<div align="center">
    <table style="width=100%">
        <tr>
            <td>
                <img src="images/spheres_1l.png" align="middle" width="480px">
            </td>
            <td>
                <img src="images/spheres_4l.png" align="middle" width="480px">
            </td>
        </tr>
        <br><br>
        <tr>
            <td>
                <img src="images/spheres_16l.png" align="middle" width="480px">
            </td>
            <td>
                <img src="images/spheres_64l.png" align="middle" width="480px">
            </td>
        </tr>
    </table>
</div>
<p>In these pictures you can see the shadows get progressively smoother as the number of light samples is increased from 1 to 4 to 16 to 64.</p>
<br>
<h2 align="middle">Part 4: Indirect Illumination</h2>
<p>I implemented the indirect lighting method according to the algorithm given in the spec. First the bsdf is sampled using the outgoing direction which returns the rsdf valuse as well as a random <code>w_in</code> and a pdf evaluated for it. I use the <code>illum()</code> function on the bsdf's returned spectrum and multiply this value by 10 and add 0.05 to it to get a probability that the ray will terminate, <code>tpdf</code>. I use this to perform a russian roulette step such that if the ray terminates that's it but if it does not terminate a new ray is created in the outgoing direction from the hit point which needs to be converted into world coordinates. Then we trace this ray to get its radiance. To get the outgoing radiance we multiply the incoming radiance by the bsdf and a cosine factor (conveniently <code>w_in.z</code> in our object coordinates). Then we divide this by the pdf of our bsdf and <code>1 - tpdf</code>. My problem with intersect prevented me from actually rendering any of these unfortunately and testing whether my implementation worked correctly.</p>
<br>
<h2 align="middle">Part 5: Adaptive Sampling</h2>
<p>Adaptive sampling allows us to speed up rendering by sampling certain parts of the image less for instance if the approximation doesn't get that much better by adding more samples. We define a cutoff that says this approximation is good enough and we don't need to sample anymore and then move on. In order to implement this I followed the provided algorithm. For each sample I accumulated the <code>illum()</code> and the <code>illum()^2</code> of the sample. Then if <code>(i+1)%spb == 0</code> where i is the current sample number and spb is the number of samples per batch I would calculate the mean and variance using the accumulated sum of <code>illum()</code> and <code>illum()^2</code> values and the formulas given in the spec. Unfortunately because my bbox intersect doesn't work direct and indirect lighting do not function perfectly and that causes adaptive sampling to also not work because the value will converge more quickly than it should in dark places because they are darker than they should be I am fairly certain that if the other parts of my project all worked properly this would also work but I was unable to test it due to bbox intersections not working.</p>
<div align="center">
    <table style="width=100%">
        <tr>
            <td>
                <img src="images/bunny.png" align="middle" width="480px">
            </td>
            <td>
                <img src="images/bunny_rate.png" align="middle" width="480px">
            </td>
        </tr>
    </table>
</div>
<p>In the above pictures you can see where particularly dark shadows that were terminated early did not normalize correctly leading to brighter than anticipated patches that did not render correctly.</p>
</div>
</body>
</html>




